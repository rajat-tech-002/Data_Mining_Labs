{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepallength  sepalwidth  petallength  petalwidth        class\n",
      "0          5.1         3.5          1.4         0.2  Iris-setosa\n",
      "1          4.9         3.0          1.4         0.2  Iris-setosa\n",
      "2          4.7         3.2          1.3         0.2  Iris-setosa\n",
      "3          4.6         3.1          1.5         0.2  Iris-setosa\n",
      "4          5.0         3.6          1.4         0.2  Iris-setosa\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iris-versicolor'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'][70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "for i in range(len(data)):\n",
    "    if data['class'][i]== 'Iris-setosa':\n",
    "       data['class'][i]=int(1)\n",
    "    elif data['class'][i]=='Iris-versicolor':\n",
    "       data['class'][i]=int(2)\n",
    "    else:\n",
    "      data['class'][i]=int(3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallength</th>\n",
       "      <th>sepalwidth</th>\n",
       "      <th>petallength</th>\n",
       "      <th>petalwidth</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallength  sepalwidth  petallength  petalwidth class\n",
       "0            5.1         3.5          1.4         0.2     1\n",
       "1            4.9         3.0          1.4         0.2     1\n",
       "2            4.7         3.2          1.3         0.2     1\n",
       "3            4.6         3.1          1.5         0.2     1\n",
       "4            5.0         3.6          1.4         0.2     1\n",
       "5            5.4         3.9          1.7         0.4     1\n",
       "6            4.6         3.4          1.4         0.3     1\n",
       "7            5.0         3.4          1.5         0.2     1\n",
       "8            4.4         2.9          1.4         0.2     1\n",
       "9            4.9         3.1          1.5         0.1     1\n",
       "10           5.4         3.7          1.5         0.2     1\n",
       "11           4.8         3.4          1.6         0.2     1\n",
       "12           4.8         3.0          1.4         0.1     1\n",
       "13           4.3         3.0          1.1         0.1     1\n",
       "14           5.8         4.0          1.2         0.2     1\n",
       "15           5.7         4.4          1.5         0.4     1\n",
       "16           5.4         3.9          1.3         0.4     1\n",
       "17           5.1         3.5          1.4         0.3     1\n",
       "18           5.7         3.8          1.7         0.3     1\n",
       "19           5.1         3.8          1.5         0.3     1\n",
       "20           5.4         3.4          1.7         0.2     1\n",
       "21           5.1         3.7          1.5         0.4     1\n",
       "22           4.6         3.6          1.0         0.2     1\n",
       "23           5.1         3.3          1.7         0.5     1\n",
       "24           4.8         3.4          1.9         0.2     1\n",
       "25           5.0         3.0          1.6         0.2     1\n",
       "26           5.0         3.4          1.6         0.4     1\n",
       "27           5.2         3.5          1.5         0.2     1\n",
       "28           5.2         3.4          1.4         0.2     1\n",
       "29           4.7         3.2          1.6         0.2     1\n",
       "..           ...         ...          ...         ...   ...\n",
       "120          6.9         3.2          5.7         2.3     3\n",
       "121          5.6         2.8          4.9         2.0     3\n",
       "122          7.7         2.8          6.7         2.0     3\n",
       "123          6.3         2.7          4.9         1.8     3\n",
       "124          6.7         3.3          5.7         2.1     3\n",
       "125          7.2         3.2          6.0         1.8     3\n",
       "126          6.2         2.8          4.8         1.8     3\n",
       "127          6.1         3.0          4.9         1.8     3\n",
       "128          6.4         2.8          5.6         2.1     3\n",
       "129          7.2         3.0          5.8         1.6     3\n",
       "130          7.4         2.8          6.1         1.9     3\n",
       "131          7.9         3.8          6.4         2.0     3\n",
       "132          6.4         2.8          5.6         2.2     3\n",
       "133          6.3         2.8          5.1         1.5     3\n",
       "134          6.1         2.6          5.6         1.4     3\n",
       "135          7.7         3.0          6.1         2.3     3\n",
       "136          6.3         3.4          5.6         2.4     3\n",
       "137          6.4         3.1          5.5         1.8     3\n",
       "138          6.0         3.0          4.8         1.8     3\n",
       "139          6.9         3.1          5.4         2.1     3\n",
       "140          6.7         3.1          5.6         2.4     3\n",
       "141          6.9         3.1          5.1         2.3     3\n",
       "142          5.8         2.7          5.1         1.9     3\n",
       "143          6.8         3.2          5.9         2.3     3\n",
       "144          6.7         3.3          5.7         2.5     3\n",
       "145          6.7         3.0          5.2         2.3     3\n",
       "146          6.3         2.5          5.0         1.9     3\n",
       "147          6.5         3.0          5.2         2.0     3\n",
       "148          6.2         3.4          5.4         2.3     3\n",
       "149          5.9         3.0          5.1         1.8     3\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      5.1\n",
       "1      4.9\n",
       "2      4.7\n",
       "3      4.6\n",
       "4      5.0\n",
       "5      5.4\n",
       "6      4.6\n",
       "7      5.0\n",
       "8      4.4\n",
       "9      4.9\n",
       "10     5.4\n",
       "11     4.8\n",
       "12     4.8\n",
       "13     4.3\n",
       "14     5.8\n",
       "15     5.7\n",
       "16     5.4\n",
       "17     5.1\n",
       "18     5.7\n",
       "19     5.1\n",
       "20     5.4\n",
       "21     5.1\n",
       "22     4.6\n",
       "23     5.1\n",
       "24     4.8\n",
       "25     5.0\n",
       "26     5.0\n",
       "27     5.2\n",
       "28     5.2\n",
       "29     4.7\n",
       "      ... \n",
       "120    6.9\n",
       "121    5.6\n",
       "122    7.7\n",
       "123    6.3\n",
       "124    6.7\n",
       "125    7.2\n",
       "126    6.2\n",
       "127    6.1\n",
       "128    6.4\n",
       "129    7.2\n",
       "130    7.4\n",
       "131    7.9\n",
       "132    6.4\n",
       "133    6.3\n",
       "134    6.1\n",
       "135    7.7\n",
       "136    6.3\n",
       "137    6.4\n",
       "138    6.0\n",
       "139    6.9\n",
       "140    6.7\n",
       "141    6.9\n",
       "142    5.8\n",
       "143    6.8\n",
       "144    6.7\n",
       "145    6.7\n",
       "146    6.3\n",
       "147    6.5\n",
       "148    6.2\n",
       "149    5.9\n",
       "Name: sepallength, Length: 150, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "data[data.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Classes-> 1-Iris-setosa 2- Iris-versicolor 3- Iris-virginica  and 4 features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614, 0.2550690257394217, 0.49543508709194095]}\n",
      "\n",
      "\n",
      "{'weights': [0.4494910647887381, 0.651592972722763, 0.7887233511355132, 0.0938595867742349, 0.02834747652200631]}\n",
      "\n",
      "\n",
      "{'weights': [0.8357651039198697, 0.43276706790505337, 0.762280082457942, 0.0021060533511106927, 0.4453871940548014]}\n",
      "\n",
      "\n",
      "{'weights': [0.7215400323407826, 0.22876222127045265, 0.9452706955539223, 0.9014274576114836, 0.030589983033553536]}\n",
      "\n",
      "\n",
      "{'weights': [0.0254458609934608, 0.5414124727934966, 0.9391491627785106, 0.38120423768821243, 0.21659939713061338]}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'weights': [0.4221165755827173, 0.029040787574867943, 0.22169166627303505, 0.43788759365057206, 0.49581224138185065, 0.23308445025757263]}\n",
      "\n",
      "\n",
      "{'weights': [0.2308665415409843, 0.2187810373376886, 0.4596034657377336, 0.28978161459048557, 0.021489705265908876, 0.8375779756625729]}\n",
      "\n",
      "\n",
      "{'weights': [0.5564543226524334, 0.6422943629324456, 0.1859062658947177, 0.9925434121760651, 0.8599465287952899, 0.12088995980580641]}\n",
      "\n",
      "\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "\tnetwork = list()\n",
    "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "\tnetwork.append(hidden_layer)\n",
    "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "\tnetwork.append(output_layer)\n",
    "\treturn network\n",
    "\n",
    "seed(1)\n",
    "network = initialize_network(4, 5, 3) # 4 feature vector as input-> hidden layer assumed 5 and output is 3 class-1,2,3\n",
    "for i in range(len(network[0])):\n",
    " print(network[0][i])\n",
    " print('\\n')\n",
    "print('\\n')\n",
    "for i in range(len(network[1])):\n",
    " print(network[1][i])\n",
    " print('\\n')\n",
    "    \n",
    "    \n",
    "for layer in network:\n",
    "    for neuron in layer:\n",
    "        print(np.shape(neuron['weights']))\n",
    "        break \n",
    "    break     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So At Hidden Layer-> 5X1 4 input+ 1 bias itself is a vector which is 1 value per neuron\n",
    "# Also at output ->  6X1 5 input+ 1 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8609707596285303, 0.8864146251436507, 0.9653897098698256]\n",
      "{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614, 0.2550690257394217, 0.49543508709194095], 'output': 0.9948674611445971, 'delta': -0.00011042865051649509}\n",
      "\n",
      "\n",
      "{'weights': [0.4494910647887381, 0.651592972722763, 0.7887233511355132, 0.0938595867742349, 0.02834747652200631], 'output': 0.996745029933176, 'delta': 7.044348179150927e-05}\n",
      "\n",
      "\n",
      "{'weights': [0.8357651039198697, 0.43276706790505337, 0.762280082457942, 0.0021060533511106927, 0.4453871940548014], 'output': 0.9993181966765046, 'delta': -7.606793540501168e-06}\n",
      "\n",
      "\n",
      "{'weights': [0.7215400323407826, 0.22876222127045265, 0.9452706955539223, 0.9014274576114836, 0.030589983033553536], 'output': 0.9975635563036737, 'delta': -1.8235811693340004e-05}\n",
      "\n",
      "\n",
      "{'weights': [0.0254458609934608, 0.5414124727934966, 0.9391491627785106, 0.38120423768821243, 0.21659939713061338], 'output': 0.9742282482444028, 'delta': -0.0005303837574198768}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'weights': [0.4221165755827173, 0.029040787574867943, 0.22169166627303505, 0.43788759365057206, 0.49581224138185065, 0.23308445025757263], 'output': 0.8609707596285303, 'delta': -0.10305829523114511}\n",
      "\n",
      "\n",
      "{'weights': [0.2308665415409843, 0.2187810373376886, 0.4596034657377336, 0.28978161459048557, 0.021489705265908876, 0.8375779756625729], 'output': 0.8864146251436507, 'delta': 0.011436200063046588}\n",
      "\n",
      "\n",
      "{'weights': [0.5564543226524334, 0.6422943629324456, 0.1859062658947177, 0.9925434121760651, 0.8599465287952899, 0.12088995980580641], 'output': 0.9653897098698256, 'delta': 0.0345688314263855}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "\tactivation = weights[-1] # Bias array's last value\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation\n",
    "\n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "\treturn 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "\tinputs = row\n",
    "\tfor layer in network:\n",
    "\t\tnew_inputs = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "\t\t\tneuron['output'] = transfer(activation)\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs\n",
    "\n",
    "# test forward propagation\n",
    "row = [5.1,3.5,1.4,0.2] #-> Class 1\n",
    "output = forward_propagate(network, row)\n",
    "print(output)\n",
    "for i in range(len(network[0])):\n",
    " print(network[0][i])\n",
    " print('\\n')\n",
    "print('\\n')\n",
    "for i in range(len(network[1])):\n",
    " print(network[1][i])\n",
    " print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614, 0.2550690257394217, 0.49543508709194095], 'output': 0.9948674611445971, 'delta': -0.00013418278548278968}\n",
      "\n",
      "\n",
      "{'weights': [0.4494910647887381, 0.651592972722763, 0.7887233511355132, 0.0938595867742349, 0.02834747652200631], 'output': 0.996745029933176, 'delta': 6.860364581729077e-05}\n",
      "\n",
      "\n",
      "{'weights': [0.8357651039198697, 0.43276706790505337, 0.762280082457942, 0.0021060533511106927, 0.4453871940548014], 'output': 0.9993181966765046, 'delta': -3.490326804112069e-05}\n",
      "\n",
      "\n",
      "{'weights': [0.7215400323407826, 0.22876222127045265, 0.9452706955539223, 0.9014274576114836, 0.030589983033553536], 'output': 0.9975635563036737, 'delta': -8.545428823696223e-06}\n",
      "\n",
      "\n",
      "{'weights': [0.0254458609934608, 0.5414124727934966, 0.9391491627785106, 0.38120423768821243, 0.21659939713061338], 'output': 0.9742282482444028, 'delta': 0.0001367049811952283}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "\treturn output * (1.0 - output)\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "\tfor i in reversed(range(len(network))):  # len(network)=2 i=1, i=0\n",
    "\t\tlayer = network[i]                    # network[1] 3X5 matrix of weights\n",
    "\t\terrors = list()                        \n",
    "\t\tif i != len(network)-1:                \n",
    "\t\t\tfor j in range(len(layer)):        # for network 0       \n",
    "\t\t\t\terror = 0.0\n",
    "\t\t\t\tfor neuron in network[i + 1]:\n",
    "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "\t\t\t\terrors.append(error)\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(len(layer)):  # network[1]  -> len(layer)=3, j=0,1,2\n",
    "\t\t\t\tneuron = layer[j]           # layer[0]\n",
    "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
    "\t\tfor j in range(len(layer)):  # j=0,1\n",
    "\t\t\tneuron = layer[j]\n",
    "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "# test backpropagation of error\n",
    "\n",
    "expected = [0, 0,3]\n",
    "backward_propagate_error(network, expected)\n",
    "for layer in network:\n",
    "    for neuron in layer:\n",
    "        print(neuron)\n",
    "        print('\\n')\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "\tfor i in range(len(network)):\n",
    "\t\tinputs = row[:-1]\n",
    "\t\tif i != 0:\n",
    "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "\t\tfor neuron in network[i]:\n",
    "\t\t\tfor j in range(len(inputs)):\n",
    "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "\t\tfor row in train:\n",
    "\t\t\toutputs = forward_propagate(network, row)\n",
    "\t\t\texpected = [0 for i in range(n_outputs)]\n",
    "\t\t\texpected[row[-1]] = 1\n",
    "\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "\t\t\tbackward_propagate_error(network, expected)\n",
    "\t\t\tupdate_weights(network, row, l_rate)\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=6.350\n",
      ">epoch=1, lrate=0.500, error=5.531\n",
      ">epoch=2, lrate=0.500, error=5.221\n",
      ">epoch=3, lrate=0.500, error=4.951\n",
      ">epoch=4, lrate=0.500, error=4.519\n",
      ">epoch=5, lrate=0.500, error=4.173\n",
      ">epoch=6, lrate=0.500, error=3.835\n",
      ">epoch=7, lrate=0.500, error=3.506\n",
      ">epoch=8, lrate=0.500, error=3.192\n",
      ">epoch=9, lrate=0.500, error=2.898\n",
      ">epoch=10, lrate=0.500, error=2.626\n",
      ">epoch=11, lrate=0.500, error=2.377\n",
      ">epoch=12, lrate=0.500, error=2.153\n",
      ">epoch=13, lrate=0.500, error=1.953\n",
      ">epoch=14, lrate=0.500, error=1.774\n",
      ">epoch=15, lrate=0.500, error=1.614\n",
      ">epoch=16, lrate=0.500, error=1.472\n",
      ">epoch=17, lrate=0.500, error=1.346\n",
      ">epoch=18, lrate=0.500, error=1.233\n",
      ">epoch=19, lrate=0.500, error=1.132\n",
      ">epoch=20, lrate=0.500, error=1.042\n",
      ">epoch=21, lrate=0.500, error=0.961\n",
      ">epoch=22, lrate=0.500, error=0.887\n",
      ">epoch=23, lrate=0.500, error=0.821\n",
      ">epoch=24, lrate=0.500, error=0.761\n",
      ">epoch=25, lrate=0.500, error=0.707\n",
      ">epoch=26, lrate=0.500, error=0.658\n",
      ">epoch=27, lrate=0.500, error=0.613\n",
      ">epoch=28, lrate=0.500, error=0.573\n",
      ">epoch=29, lrate=0.500, error=0.536\n",
      ">epoch=30, lrate=0.500, error=0.503\n",
      ">epoch=31, lrate=0.500, error=0.472\n",
      ">epoch=32, lrate=0.500, error=0.445\n",
      ">epoch=33, lrate=0.500, error=0.420\n",
      ">epoch=34, lrate=0.500, error=0.397\n",
      ">epoch=35, lrate=0.500, error=0.376\n",
      ">epoch=36, lrate=0.500, error=0.356\n",
      ">epoch=37, lrate=0.500, error=0.339\n",
      ">epoch=38, lrate=0.500, error=0.322\n",
      ">epoch=39, lrate=0.500, error=0.307\n",
      ">epoch=40, lrate=0.500, error=0.293\n",
      ">epoch=41, lrate=0.500, error=0.280\n",
      ">epoch=42, lrate=0.500, error=0.268\n",
      ">epoch=43, lrate=0.500, error=0.257\n",
      ">epoch=44, lrate=0.500, error=0.247\n",
      ">epoch=45, lrate=0.500, error=0.237\n",
      ">epoch=46, lrate=0.500, error=0.228\n",
      ">epoch=47, lrate=0.500, error=0.220\n",
      ">epoch=48, lrate=0.500, error=0.212\n",
      ">epoch=49, lrate=0.500, error=0.204\n",
      ">epoch=50, lrate=0.500, error=0.197\n",
      ">epoch=51, lrate=0.500, error=0.191\n",
      ">epoch=52, lrate=0.500, error=0.185\n",
      ">epoch=53, lrate=0.500, error=0.179\n",
      ">epoch=54, lrate=0.500, error=0.173\n",
      ">epoch=55, lrate=0.500, error=0.168\n",
      ">epoch=56, lrate=0.500, error=0.163\n",
      ">epoch=57, lrate=0.500, error=0.158\n",
      ">epoch=58, lrate=0.500, error=0.154\n",
      ">epoch=59, lrate=0.500, error=0.150\n",
      ">epoch=60, lrate=0.500, error=0.146\n",
      ">epoch=61, lrate=0.500, error=0.142\n",
      ">epoch=62, lrate=0.500, error=0.138\n",
      ">epoch=63, lrate=0.500, error=0.135\n",
      ">epoch=64, lrate=0.500, error=0.131\n",
      ">epoch=65, lrate=0.500, error=0.128\n",
      ">epoch=66, lrate=0.500, error=0.125\n",
      ">epoch=67, lrate=0.500, error=0.122\n",
      ">epoch=68, lrate=0.500, error=0.119\n",
      ">epoch=69, lrate=0.500, error=0.117\n",
      ">epoch=70, lrate=0.500, error=0.114\n",
      ">epoch=71, lrate=0.500, error=0.112\n",
      ">epoch=72, lrate=0.500, error=0.109\n",
      ">epoch=73, lrate=0.500, error=0.107\n",
      ">epoch=74, lrate=0.500, error=0.105\n",
      ">epoch=75, lrate=0.500, error=0.103\n",
      ">epoch=76, lrate=0.500, error=0.101\n",
      ">epoch=77, lrate=0.500, error=0.099\n",
      ">epoch=78, lrate=0.500, error=0.097\n",
      ">epoch=79, lrate=0.500, error=0.095\n",
      ">epoch=80, lrate=0.500, error=0.093\n",
      ">epoch=81, lrate=0.500, error=0.092\n",
      ">epoch=82, lrate=0.500, error=0.090\n",
      ">epoch=83, lrate=0.500, error=0.088\n",
      ">epoch=84, lrate=0.500, error=0.087\n",
      ">epoch=85, lrate=0.500, error=0.085\n",
      ">epoch=86, lrate=0.500, error=0.084\n",
      ">epoch=87, lrate=0.500, error=0.083\n",
      ">epoch=88, lrate=0.500, error=0.081\n",
      ">epoch=89, lrate=0.500, error=0.080\n",
      ">epoch=90, lrate=0.500, error=0.079\n",
      ">epoch=91, lrate=0.500, error=0.077\n",
      ">epoch=92, lrate=0.500, error=0.076\n",
      ">epoch=93, lrate=0.500, error=0.075\n",
      ">epoch=94, lrate=0.500, error=0.074\n",
      ">epoch=95, lrate=0.500, error=0.073\n",
      ">epoch=96, lrate=0.500, error=0.072\n",
      ">epoch=97, lrate=0.500, error=0.071\n",
      ">epoch=98, lrate=0.500, error=0.070\n",
      ">epoch=99, lrate=0.500, error=0.069\n",
      ">epoch=100, lrate=0.500, error=0.068\n",
      ">epoch=101, lrate=0.500, error=0.067\n",
      ">epoch=102, lrate=0.500, error=0.066\n",
      ">epoch=103, lrate=0.500, error=0.065\n",
      ">epoch=104, lrate=0.500, error=0.064\n",
      ">epoch=105, lrate=0.500, error=0.063\n",
      ">epoch=106, lrate=0.500, error=0.062\n",
      ">epoch=107, lrate=0.500, error=0.062\n",
      ">epoch=108, lrate=0.500, error=0.061\n",
      ">epoch=109, lrate=0.500, error=0.060\n",
      ">epoch=110, lrate=0.500, error=0.059\n",
      ">epoch=111, lrate=0.500, error=0.059\n",
      ">epoch=112, lrate=0.500, error=0.058\n",
      ">epoch=113, lrate=0.500, error=0.057\n",
      ">epoch=114, lrate=0.500, error=0.057\n",
      ">epoch=115, lrate=0.500, error=0.056\n",
      ">epoch=116, lrate=0.500, error=0.055\n",
      ">epoch=117, lrate=0.500, error=0.055\n",
      ">epoch=118, lrate=0.500, error=0.054\n",
      ">epoch=119, lrate=0.500, error=0.053\n",
      ">epoch=120, lrate=0.500, error=0.053\n",
      ">epoch=121, lrate=0.500, error=0.052\n",
      ">epoch=122, lrate=0.500, error=0.052\n",
      ">epoch=123, lrate=0.500, error=0.051\n",
      ">epoch=124, lrate=0.500, error=0.050\n",
      ">epoch=125, lrate=0.500, error=0.050\n",
      ">epoch=126, lrate=0.500, error=0.049\n",
      ">epoch=127, lrate=0.500, error=0.049\n",
      ">epoch=128, lrate=0.500, error=0.048\n",
      ">epoch=129, lrate=0.500, error=0.048\n",
      ">epoch=130, lrate=0.500, error=0.047\n",
      ">epoch=131, lrate=0.500, error=0.047\n",
      ">epoch=132, lrate=0.500, error=0.046\n",
      ">epoch=133, lrate=0.500, error=0.046\n",
      ">epoch=134, lrate=0.500, error=0.046\n",
      ">epoch=135, lrate=0.500, error=0.045\n",
      ">epoch=136, lrate=0.500, error=0.045\n",
      ">epoch=137, lrate=0.500, error=0.044\n",
      ">epoch=138, lrate=0.500, error=0.044\n",
      ">epoch=139, lrate=0.500, error=0.043\n",
      ">epoch=140, lrate=0.500, error=0.043\n",
      ">epoch=141, lrate=0.500, error=0.043\n",
      ">epoch=142, lrate=0.500, error=0.042\n",
      ">epoch=143, lrate=0.500, error=0.042\n",
      ">epoch=144, lrate=0.500, error=0.041\n",
      ">epoch=145, lrate=0.500, error=0.041\n",
      ">epoch=146, lrate=0.500, error=0.041\n",
      ">epoch=147, lrate=0.500, error=0.040\n",
      ">epoch=148, lrate=0.500, error=0.040\n",
      ">epoch=149, lrate=0.500, error=0.040\n",
      ">epoch=150, lrate=0.500, error=0.039\n",
      ">epoch=151, lrate=0.500, error=0.039\n",
      ">epoch=152, lrate=0.500, error=0.039\n",
      ">epoch=153, lrate=0.500, error=0.038\n",
      ">epoch=154, lrate=0.500, error=0.038\n",
      ">epoch=155, lrate=0.500, error=0.038\n",
      ">epoch=156, lrate=0.500, error=0.037\n",
      ">epoch=157, lrate=0.500, error=0.037\n",
      ">epoch=158, lrate=0.500, error=0.037\n",
      ">epoch=159, lrate=0.500, error=0.037\n",
      ">epoch=160, lrate=0.500, error=0.036\n",
      ">epoch=161, lrate=0.500, error=0.036\n",
      ">epoch=162, lrate=0.500, error=0.036\n",
      ">epoch=163, lrate=0.500, error=0.035\n",
      ">epoch=164, lrate=0.500, error=0.035\n",
      ">epoch=165, lrate=0.500, error=0.035\n",
      ">epoch=166, lrate=0.500, error=0.035\n",
      ">epoch=167, lrate=0.500, error=0.034\n",
      ">epoch=168, lrate=0.500, error=0.034\n",
      ">epoch=169, lrate=0.500, error=0.034\n",
      ">epoch=170, lrate=0.500, error=0.034\n",
      ">epoch=171, lrate=0.500, error=0.033\n",
      ">epoch=172, lrate=0.500, error=0.033\n",
      ">epoch=173, lrate=0.500, error=0.033\n",
      ">epoch=174, lrate=0.500, error=0.033\n",
      ">epoch=175, lrate=0.500, error=0.032\n",
      ">epoch=176, lrate=0.500, error=0.032\n",
      ">epoch=177, lrate=0.500, error=0.032\n",
      ">epoch=178, lrate=0.500, error=0.032\n",
      ">epoch=179, lrate=0.500, error=0.032\n",
      ">epoch=180, lrate=0.500, error=0.031\n",
      ">epoch=181, lrate=0.500, error=0.031\n",
      ">epoch=182, lrate=0.500, error=0.031\n",
      ">epoch=183, lrate=0.500, error=0.031\n",
      ">epoch=184, lrate=0.500, error=0.030\n",
      ">epoch=185, lrate=0.500, error=0.030\n",
      ">epoch=186, lrate=0.500, error=0.030\n",
      ">epoch=187, lrate=0.500, error=0.030\n",
      ">epoch=188, lrate=0.500, error=0.030\n",
      ">epoch=189, lrate=0.500, error=0.029\n",
      ">epoch=190, lrate=0.500, error=0.029\n",
      ">epoch=191, lrate=0.500, error=0.029\n",
      ">epoch=192, lrate=0.500, error=0.029\n",
      ">epoch=193, lrate=0.500, error=0.029\n",
      ">epoch=194, lrate=0.500, error=0.029\n",
      ">epoch=195, lrate=0.500, error=0.028\n",
      ">epoch=196, lrate=0.500, error=0.028\n",
      ">epoch=197, lrate=0.500, error=0.028\n",
      ">epoch=198, lrate=0.500, error=0.028\n",
      ">epoch=199, lrate=0.500, error=0.028\n",
      "[{'weights': [-1.9840720697674477, 2.753255094433545, 1.427688378413599], 'output': 0.015845027117728623, 'delta': -0.0002410319781990047}, {'weights': [1.1390666350663963, -1.692904205238224, -0.3243968738600616], 'output': 0.9210833874002823, 'delta': 0.0005458208693962861}]\n",
      "[{'weights': [4.928084483306895, -2.0628451404591988, -1.314165888192279], 'output': 0.041702700400936064, 'delta': -0.0016665894197174123}, {'weights': [-4.5840586831162655, 2.5778944975940523, 0.8626042569273801], 'output': 0.959416547504382, 'delta': 0.0015801749958503201}]\n"
     ]
    }
   ],
   "source": [
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 200, n_outputs)\n",
    "for layer in network:\n",
    "\tprint(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-542d7b8e0047>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'type'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[sepallength    5.1\n",
       " sepalwidth     3.5\n",
       " petallength    1.4\n",
       " petalwidth     0.2\n",
       " class            1\n",
       " Name: 0, dtype: object]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=list()\n",
    "for i in range(len(data)):\n",
    "    dataset.append([data.iloc[i]])\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
